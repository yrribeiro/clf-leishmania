{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from scipy.stats import mode\n",
    "from pytorch_metric_learning import testers\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "TRAIN_SIZE = .7\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_CLASSES = 2\n",
    "IMG_SIZE = (1, 96, 96)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extractor class (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc_input_size = self.calculate_fc_input_size(IMG_SIZE)\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def calculate_fc_input_size(self, input_size):\n",
    "        x = torch.randn(1, *input_size)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for simultaneous reading and splitting data from folders\n",
    "Expected folder structure:\n",
    "<br>\n",
    "\n",
    "dataset<br>\n",
    "...|___ leish<br>\n",
    "...|___ no-leish<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(data_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((96, 96)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "\n",
    "    train_size = int(TRAIN_SIZE * len(dataset))\n",
    "    # test_size = len(dataset) - train_size\n",
    "    indices = torch.randperm(len(dataset)).tolist()\n",
    "    train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Obtendo as etiquetas verdadeiras do test_dataset usando Subset\n",
    "    train_true_labels = [dataset.targets[idx] for idx in train_indices]\n",
    "    test_true_labels = [dataset.targets[idx] for idx in test_indices]\n",
    "\n",
    "    leish_train = sum(label == 0 for _, label in train_dataset)\n",
    "    leish_test = sum(label == 0 for _, label in test_dataset)\n",
    "\n",
    "    print(f'label format = {dataset.class_to_idx}')\n",
    "    print(f'train test split proportion = train[{len(train_dataset)}], test[{len(test_dataset)}]')\n",
    "    print(f'leish in training set = {leish_train}')\n",
    "    print(f'leish in testing set = {leish_test}')\n",
    "\n",
    "    return train_dataset, test_dataset, train_loader, test_loader, train_true_labels, test_true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading pre-trained models with respectives StandardScaler, PCA and SVM\n",
    "ALL MODELS SHOULD BE USED FROM \\v1-ensble_2023-11-07\\ <br>\n",
    "\n",
    "nome do arquivo zip = v1-ensble_2023-11-07.zip <br>\n",
    "!gdown 1hVZE8yOAL6txiscunA5egvErwfBxZaEc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = Net(EMBEDDING_SIZE).to(device)\n",
    "triplet_model.load_state_dict(torch.load('./models/v1-ensble_2023-11-07\\model_Triplet_v1-ensble_2023-11-07.pth'))\n",
    "\n",
    "triplet_model.eval()\n",
    "\n",
    "triplet_clf = load('./models/v1-ensble_2023-11-07\\clf_Triplet_v1-ensble_2023-11-07.joblib')\n",
    "triplet_scaler = load('./models/v1-ensble_2023-11-07\\scaler_Triplet_v1-ensble_2023-11-07.joblib')\n",
    "triplet_pca = load('./models/v1-ensble_2023-11-07\\pca_Triplet_v1-ensble_2023-11-07.joblib')\n",
    "\n",
    "################################\n",
    "\n",
    "cosface_model = Net(EMBEDDING_SIZE).to(device)\n",
    "cosface_model.load_state_dict(torch.load('./models/v1-ensble_2023-11-07\\model_CosFace_v1-ensble_2023-11-07.pth'))\n",
    "\n",
    "cosface_model.eval()\n",
    "\n",
    "cosface_clf = load('./models/v1-ensble_2023-11-07\\clf_CosFace_v1-ensble_2023-11-07.joblib')\n",
    "cosface_scaler = load('./models/v1-ensble_2023-11-07\\scaler_CosFace_v1-ensble_2023-11-07.joblib')\n",
    "cosface_pca = load('./models/v1-ensble_2023-11-07\\pca_CosFace_v1-ensble_2023-11-07.joblib')\n",
    "\n",
    "################################\n",
    "\n",
    "multisim_model = Net(EMBEDDING_SIZE).to(device)\n",
    "multisim_model.load_state_dict(torch.load('./models/v1-ensble_2023-11-07\\model_MultiSimilarity_v1-ensble_2023-11-07.pth'))\n",
    "\n",
    "multisim_model.eval()\n",
    "\n",
    "multisim_clf = load('./models/v1-ensble_2023-11-07\\clf_MultiSimilarity_v1-ensble_2023-11-07.joblib')\n",
    "multisim_scaler = load('./models/v1-ensble_2023-11-07\\scaler_MultiSimilarity_v1-ensble_2023-11-07.joblib')\n",
    "multisim_pca = load('./models/v1-ensble_2023-11-07\\pca_MultiSimilarity_v1-ensble_2023-11-07.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ensembling functions (technique: majority voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "def get_predictions(x_test, models, classifiers, scalers, pcas):\n",
    "    '''\n",
    "    Função para pré-processar e obter previsões de cada classificador\n",
    "    com base nas embeddings extraídas de cada modelo correspondente.\n",
    "    '''\n",
    "    predictions = []\n",
    "    for model, clf, scaler, pca in zip(models, classifiers, scalers, pcas):\n",
    "        test_embed, _ = get_all_embeddings(x_test, model)\n",
    "        test_embed_scaled = scaler.transform(test_embed.cpu().numpy())\n",
    "        test_embed_pca = pca.transform(test_embed_scaled)\n",
    "\n",
    "        preds = clf.predict(test_embed_pca)\n",
    "        predictions.append(preds)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def combine_predictions(predictions):\n",
    "    '''\n",
    "    Função para combinar previsões usando votação majoritária\n",
    "    Axis=0 (col) para votação por amostra\n",
    "    '''\n",
    "    return mode(predictions, axis=0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label format = {'leish': 0, 'no-leish': 1}\n",
      "train test split proportion = train[7357], test[3154]\n",
      "leish in training set = 5152\n",
      "leish in testing set = 2189\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('./data-9500train/all_patches/') # <--- SUBSTITUA O DATASET\n",
    "x_train, x_test, x_train_loader, x_test_loader, y_true_train, y_true_test = read_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:13<00:00,  7.18it/s]\n",
      "100%|██████████| 99/99 [00:07<00:00, 13.16it/s]\n",
      "100%|██████████| 99/99 [00:07<00:00, 13.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Ensemble result for Triplet, Cosface, Multisimilarity\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Leish       0.97      0.94      0.95      2189\n",
      "    No Leish       0.87      0.93      0.90       965\n",
      "\n",
      "    accuracy                           0.94      3154\n",
      "   macro avg       0.92      0.93      0.93      3154\n",
      "weighted avg       0.94      0.94      0.94      3154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = [triplet_clf, cosface_clf, multisim_clf]\n",
    "scalers = [triplet_scaler, cosface_scaler,  multisim_scaler]\n",
    "pcas = [triplet_pca, cosface_pca, multisim_pca]\n",
    "models = [triplet_model, cosface_model, multisim_model]\n",
    "\n",
    "individual_preds = get_predictions(x_test, models, classifiers, scalers, pcas)\n",
    "ensemble_prediction = combine_predictions(individual_preds)\n",
    "print('---- Ensemble result for Triplet, Cosface, Multisimilarity')\n",
    "print(classification_report(y_true_test, ensemble_prediction, target_names=['Leish', 'No Leish']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
